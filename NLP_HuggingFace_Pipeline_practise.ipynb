{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPf/vW+elNxjX1rQXGQleub"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th7vPz61qQ4W"
      },
      "outputs": [],
      "source": [
        "!pip install transformers pipelines datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "ZNfJdW63qhkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text classification\n",
        "Text classification involves categorizing text into predefined categories or labels. It is commonly used for sentiment analysis, spam detection, and topic classification."
      ],
      "metadata": {
        "id": "HJrEAVO6qqTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a pre-trained text classification model\n",
        "classifier = pipeline('sentiment-analysis')\n",
        "\n",
        "# Example text for classification\n",
        "text_to_classify = \"I really enjoyed the movie. The plot was intriguing, and the acting was superb.\"\n",
        "\n",
        "# Perform text classification\n",
        "classification_result = classifier(text_to_classify)\n",
        "\n",
        "# Display the result\n",
        "print(f\"Text: {text_to_classify}\")\n",
        "print(f\"Predicted Sentiment: {classification_result[0]['label']} with confidence: {classification_result[0]['score']}\")"
      ],
      "metadata": {
        "id": "JZXanl3DqyYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text classification-NER\n",
        "Token classification involves assigning a label to each token in a sequence. It is commonly used for Named Entity Recognition (NER) tasks."
      ],
      "metadata": {
        "id": "xmFEOkJirEo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the NER pipeline\n",
        "ner_pipeline = pipeline('ner')\n",
        "\n",
        "# Example text for named entity recognition\n",
        "sample_text = \"Hugging Face is a fantastic platform for NLP tasks. It was founded by researchers in 2016.\"\n",
        "\n",
        "# Perform named entity recognition\n",
        "ner_results = ner_pipeline(sample_text)\n",
        "\n",
        "# Display the result\n",
        "print(f\"Sample Text: {sample_text}\")\n",
        "print(\"Named Entities:\")\n",
        "for entity in ner_results:\n",
        "    print(f\"Entity: {entity['word']}, Label: {entity['entity']}, Score: {entity['score']}\")"
      ],
      "metadata": {
        "id": "PM-BsMH7q_yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table Question Answering\n",
        "Table question answering involves extracting information from tabular data. The model takes a question and a table as input and provides an answer."
      ],
      "metadata": {
        "id": "MhRCu-8ArTqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# prepare table + question\n",
        "data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n",
        "table = pd.DataFrame.from_dict(data)\n",
        "question = \"how many movies does Leonardo Di Caprio have?\"\n",
        "\n",
        "# pipeline model\n",
        "# Note: you must to install torch-scatter first.\n",
        "tqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n",
        "\n",
        "# result\n",
        "\n",
        "print(tqa(table=table, query=question)['cells'][0])"
      ],
      "metadata": {
        "id": "xE1mdNK-rLop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normal Question Answer\n",
        "Question answering involves providing an answer to a question based on a given context. The model takes a question and a context as input and outputs the answer."
      ],
      "metadata": {
        "id": "GaqdcZ96riat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the question answering pipeline\n",
        "qa_pipeline = pipeline('question-answering')\n",
        "\n",
        "# Example context and question for QA\n",
        "context = \"Hugging Face is a popular platform for natural language processing. It was founded in 2016.\"\n",
        "question = \"When was Hugging Face founded?\"\n",
        "\n",
        "# Perform question answering\n",
        "qa_result = qa_pipeline(question=question, context=context)\n",
        "\n",
        "# Display the answer\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {qa_result['answer']}\")"
      ],
      "metadata": {
        "id": "-lNcUeDirj0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example using llama 2 model with context based question answering\n",
        "\n",
        "- For more advanced techniques we use libraries like LangChain for building a complete question answering system with LLama-2."
      ],
      "metadata": {
        "id": "zYG4-3SN75dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load question answering pipeline with LLaMA-2 7B model\n",
        "question_answering = pipeline(\n",
        "    task=\"question-answering\",\n",
        "    model=\"meta-llama/Llama-2-7b-chat-hf\"\n",
        ")\n",
        "\n",
        "# Define your document corpus (replace with your data source)\n",
        "documents = [\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"London is the capital of England.\",\n",
        "    \"Mount Everest is the tallest mountain in the world.\"\n",
        "]\n",
        "\n",
        "# Ask a question\n",
        "question = \"What is the capital of France?\"\n",
        "\n",
        "# Get the answer using the pipeline\n",
        "answer = question_answering(question=question, context=documents)\n",
        "\n",
        "# Print the answer\n",
        "print(f\"Answer: {answer['answer']}\")\n",
        "print(f\"Confidence score: {answer['score']}\")\n"
      ],
      "metadata": {
        "id": "lkoOZ7z372np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-Shot Classification\n",
        "Zero-shot classification involves classifying text into predefined categories without any training examples for those categories.\n"
      ],
      "metadata": {
        "id": "XYPwuuN3robk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the zero-shot classification pipeline\n",
        "zero_shot_classifier = pipeline('zero-shot-classification')\n",
        "\n",
        "# Example text for zero-shot classification\n",
        "text_to_classify = \"The new smartphone has impressive camera features.\"\n",
        "\n",
        "# Specify candidate labels\n",
        "candidate_labels = [\"Technology\", \"Fashion\", \"Sports\"]\n",
        "\n",
        "# Perform zero-shot classification\n",
        "classification_result = zero_shot_classifier(text_to_classify, candidate_labels)\n",
        "\n",
        "# Display the result\n",
        "print(f\"Text: {text_to_classify}\")\n",
        "print(f\"Predicted Label: {classification_result['labels'][0]} with confidence: {classification_result['scores'][0]}\")"
      ],
      "metadata": {
        "id": "YC6IMDzfroDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Translation\n",
        "Translation involves converting text from one language to another. The model takes a piece of text in one language as input and provides the translated text."
      ],
      "metadata": {
        "id": "zRlnjAvkr0Eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the translation pipeline\n",
        "translator = pipeline('translation', model='Helsinki-NLP/opus-mt-en-fr')\n",
        "\n",
        "# Example text for translation\n",
        "text_to_translate = \"Hello, how are you?\"\n",
        "\n",
        "# Perform translation\n",
        "translation_result = translator(text_to_translate)\n",
        "\n",
        "# Display the translated text\n",
        "print(f\"Text to Translate: {text_to_translate}\")\n",
        "print(f\"Translated Text: {translation_result[0]['translation_text']}\")"
      ],
      "metadata": {
        "id": "IFrVAZIlr4LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarization\n",
        "Summarization involves condensing a piece of text into a shorter version while retaining the essential information."
      ],
      "metadata": {
        "id": "_el6aHokr684"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the summarization pipeline\n",
        "summarizer = pipeline('summarization')\n",
        "\n",
        "# Example text for summarization\n",
        "text_to_summarize = \"Hugging Face Transformers provide a versatile interface for various NLP tasks. The library simplifies the implementation of state-of-the-art models.\"\n",
        "\n",
        "# Perform summarization\n",
        "summary_result = summarizer(text_to_summarize, max_length=50, min_length=25) # these params can be adjusted\n",
        "\n",
        "# Display the summarized text\n",
        "print(f\"Original Text: {text_to_summarize}\")\n",
        "print(f\"Summarized Text: {summary_result[0]['summary_text']}\")"
      ],
      "metadata": {
        "id": "viqdvngsr9jd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}